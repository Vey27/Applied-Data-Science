# -*- coding: utf-8 -*-
"""adsP1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x-BAdG2CS1F9K0ESBDYkoMrYLk6RwkHg

#Applied Data Science with Python
###Feature Engineering:

**After reviewing my knowledge of statistics and my introduction to Artificial Intelligence and machine learning, I am excited to embark on my first real project in Applied Data Science with Python. This project will allow me to apply my Python skills in a practical and hands-on manner, focusing on feature engineering techniques in the context of data analysis and machine learning.**

Python feature engineering refers to the process of transforming and manipulating raw data into a format that can be effectively utilized by machine learning models. It involves selecting, creating, and transforming features to improve the performance and accuracy of predictive models.

**Some common techniques and features used in Python feature engineering include:**

1. Imputation: Handling missing data by filling in or estimating the missing values.

2. Encoding: Converting categorical variables into numerical representations that can be understood by machine learning algorithms. This includes techniques like one-hot encoding, label encoding, and ordinal encoding.

3. Scaling and normalization: Rescaling numerical features to ensure they are on a similar scale, which helps in improving the performance of certain machine learning algorithms.

4. Feature extraction: Creating new features from existing ones to capture more relevant information. This can involve techniques like extracting date/time components, generating statistical features, or performing dimensionality reduction through methods like principal component analysis (PCA).

5. Text processing: Preprocessing and transforming text data by techniques like tokenization, stemming, lemmatization, and feature extraction using methods like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings.

6. Handling outliers: Identifying and handling outliers in the data to avoid their negative impact on model performance.

7. Feature selection: Selecting the most relevant features that contribute significantly to the predictive power of the model while reducing complexity and overfitting. Techniques like correlation analysis, feature importance ranking, and recursive feature elimination can be used.

Using various libraries and frameworks like pandas, scikit-learn, and NumPy that offer a wide range of tools and functions to perform feature engineering tasks efficiently.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
# Import label encoder
from sklearn import preprocessing

df = pd.read_csv('PEP1.csv')
df

df.describe()

"""**'df.sescribe'**, provides a summary statistical measures for each column in the DataFrame, such as count, mean, standard deviation, minimum value, 25th percentile, median (50th percentile), 75th percentile, and maximum value.

##1.	Understand the dataset:
- a.	Identify the shape of the dataset
- b.	Identify variables with null values
- c.	Identify variables with unique values

The 'df.columns' attribute is used to retrieve the column names of a DataFrame, providing insights into the titles and the data they represent. By accessing 'df.columns', you can obtain a list or Index object that contains the names of all the columns present in the DataFrame. 
This attribute allows you to conveniently access the column labels and use them for various purposes, such as selecting specific columns, renaming columns, or performing operations on individual or multiple columns.
"""

df.columns

"""###a.	Identify the shape of the dataset

"""

df.shape

"""- There are 1460 records or instances (rows) in the DataFrame.
- There are 81 variables or features (columns) in the DataFrame.

- 'True' represents a missing or NaN value.
- 'False' represents a non-missing or non-NaN value.

###b.	Identify variables with null values
"""

#finding missing values
df.isna().any()

df.isna().sum()

"""- The "Id," "MSSubClass," "MSZoning," "LotArea," and other columns have a value of 0, which means there are no missing values in those columns.
- The "LotFrontage" column has a value of 259, indicating that there are 259 missing values in that column.
"""

df.count()

df.drop_duplicates()

df.sort_values(['LotArea','SalePrice'],ascending=[True,False])

"""###c. Identify variables with unique values

"""

df['SalePrice'].unique()[:20]

df.SalePrice.value_counts()

top_10_prices = df.SalePrice.value_counts().nlargest(10)

plt.figure(figsize=(8, 4))
sns.barplot(x=top_10_prices.index, y=top_10_prices.values, color='blue')
plt.title('Top 10 Most Frequent Sale Prices', fontsize=16)
plt.xlabel('Sale Price', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

df['SalePrice'].value_counts(normalize=True)

df['LotArea'].unique()[:20]

"""- **'SalePrice'** Provide insights into the distribution and range of values present in the 'SalePrice' column
- Obtain an array or list containing the first 20 unique values found in the **'LotFrontage'** column of the DataFrame df. This can provide insights into the range of lot frontage values in the dataset.
"""

df['LotArea'].value_counts()

top_10_lot_area = df['LotArea'].value_counts().nlargest(10)

plt.figure(figsize=(8, 4))
sns.barplot(x=top_10_lot_area.index, y=top_10_lot_area.values, color='blue')
plt.title('Top 10 Sales based on Lot Area', fontsize=16)
plt.xlabel('Lot Area', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.show()

plt.figure(figsize=(10, 6))
sns.scatterplot(x='LotArea', y='SalePrice', data=df, color='blue', alpha=0.6)
plt.title('Lot Area vs. Sale Price', fontsize=16)
plt.xlabel('Lot Area', fontsize=12)
plt.ylabel('Sale Price', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

"""The scatter plot reveals the data points are densely clustered within the range of "LotArea" between 1000 and 2000, and the corresponding sale prices are mostly between $90K and $350K. However, there are a few outliers that deviate from this trend, particularly for lot areas above 50000 and corresponding sale prices.

# 2. Generate a separate dataset for numerical and categorical variables
"""

numerical = df.select_dtypes(include=[np.number])
categorical = df.select_dtypes(exclude=[np.number])

numericcol=numerical.columns.tolist()
categorycol=categorical.columns.tolist()
print ("Category :",categorycol)
print("\n Numeric:", numericcol)

df.tail()

"""Executing **'df.tail()'** will output the last 5 rows and 81 columns of the DataFrame df. Each row will display the values of all columns, checking for any patterns or trends, or simply reviewing the overall structure of the DataFrame

#3. Exploratory Data Analysis (EDA)of numerical variables:

- Missing value treatment
- Identify the skewness and distribution
- Identify significant variables using a correlation matrix
- Pair plot for distribution and density
"""

numerical.head()

"""###a.	Missing value treatment

"""

# check if there is a missing value in the data
numerical.isna().any()

# check if there is a missing value in the data
numerical.isna().any()

numerical.shape

# dropping the rows where we have a missing value
numerical_cleaned=numerical.dropna()

numerical_cleaned.shape

numerical.describe()

"""### b.	Identify the skewness and distribution"""

df.skew(skipna=True)

"""The skewness value indicates the degree of skewness in the distribution. A positive skewness value means the distribution is skewed to the right (long tail on the right), while a negative skewness value means the distribution is skewed to the left (long tail on the left).

- **"Id 0.000000"** suggests that the 'Id' column has a skewness of 0, indicating a symmetric distribution.
- **"LotArea 12.207688"** indicates that the 'LotArea' column has a high positive skewness, suggesting a right-skewed distribution with a long tail on the right.
"""

fig = plt.figure(figsize =(12, 4))

df.skew().plot()

fig, ax = plt.subplots(figsize=(12, 8))

# Plot the skewness values
#df.skew().plot(kind='bar', color='#457B9D', alpha=0.8)
df.skew().plot(kind='bar', ax=ax, color='darkblue', width=0.6)

# Customize the plot
ax.set_title('Skewness of Variables', fontsize=16)
ax.set_xlabel('Variables', fontsize=12)
ax.set_ylabel('Skewness', fontsize=12)
ax.tick_params(labelsize=10)
ax.grid(axis='y', linestyle='--', alpha=0.5)

# Remove spines
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

# Add value labels to each bar
for p in ax.patches:
    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2, p.get_height()),
                ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt

numerical.var()

numerical.head()

numerical.corr(method='pearson')

# Display correlation in matrix in heatmap
plt.figure(figsize=(20,10))
# Plot the correlation heatmap
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
#sns.heatmap(numerical.corr(),annot=True,cmap='viridis')
plt.show()

#saleprice correlation matrix
k = 10 #number of variables for heatmap
cols = df.corr().nlargest(k, 'SalePrice')['SalePrice'].index
cm = df[cols].corr()
plt.figure(figsize=(10,6))
sns.heatmap(cm, annot=True, cmap = 'viridis')

"""#c.	Identify significant variables using a correlation matrix

Identify variables that have a strong positive or negative correlation. Variables with high positive correlation tend to move together, while those with high negative correlation tend to move in opposite directions. This information is useful for understanding relationships between variables, identifying potential multicollinearity, and selecting variables for further analysis or modeling.

Color scale: The heatmap uses a color scale to represent the correlation values. A gradient color scheme is used, with different colors indicating different levels of correlation. For example, a gradient from light colors (e.g., light blue) to dark colors (e.g., dark red) can represent a range from negative to positive correlations.

#d.	Pair plot for distribution and density
"""

num_cols = ['LotArea', 'SalePrice']
colors = ['#FF6F61', '#6FA8FF']  # Customized colors for the histograms

for i in range(0, len(num_cols), 2):
    plt.figure(figsize=(10, 4))
    plt.subplot(121)
    sns.distplot(df[num_cols[i]], hist=True, kde=True, color=colors[0])
    plt.subplot(122)
    sns.distplot(df[num_cols[i+1]], hist=True, kde=True, color=colors[1])
    plt.tight_layout()
    plt.show()

"""# 4.	EDA of categorical variables

### a.	Missing value treatment
"""

# check if there is a missing value in the data
categorical.isna().any()

categorical.isna().sum()*100/df.shape[0]

"""- Alley: 93.77% of the values in the "Alley" column are missing.
- MasVnrType: 0.55% of the values in the "MasVnrType" column are missing.
- BsmtQual: 2.53% of the values in the "BsmtQual" column are missing.
- BsmtCond: 2.53% of the values in the "BsmtCond" column are missing.
- BsmtExposure: 2.60% of the values in the "BsmtExposure" column are missing.
- BsmtFinType1: 2.53% of the values in the "BsmtFinType1" column are missing.
- BsmtFinType2: 2.60% of the values in the "BsmtFinType2" column are missing.
"""

categorical.shape

# dropping the rows where we have a missing value
dfC=numerical.dropna()

dfC.shape

dfC.describe()



"""###b. Count plot and box plot for bivariate analysis"""

#Countplot for Categorical Feature 'SalePrice'

plt.figure(figsize=(10,5))
sns.countplot(x='SalePrice', data=dfC, order=dfC['SalePrice'].value_counts().head(20).index, palette='magma')
plt.xlabel('Sale Price', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.title('Top 20 Sale Prices', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.show()

"""The top 20 houses with a sale price of 140,000 have the highest number of sales, while the lowest sales are observed for houses with sale prices between 120,000 and $115,000."""

plt.figure(figsize=(10, 4))
sns.regplot(x='LotArea', y='SalePrice', data=dfC, scatter_kws={'alpha':0.5}, color='blue', line_kws={'color':'orange'})
plt.xlabel('Lot Area', fontsize=12)
plt.ylabel('Sale Price', fontsize=12)
plt.title('Relationship between Lot Area and Sale Price', fontsize=16)
plt.show()

"""The plot showcases the relationship between the size of the land ("Lot Area") and the sale price of houses. It is presented as a scatter plot, where each data point represents a house, with the x-axis representing the lot area and the y-axis representing the sale price. Additionally, there is an orange regression line that indicates the overall trend in the data. From the plot, we can observe that there is a higher density of lot areas between 10,000 and 35,000.Notice there is one outlier 'LotArea' sold above 200,000."""

#Countplot for Categorical Feature 'LotArea'
plt.figure(figsize=(10, 4))
sns.countplot(y='LotArea', data=dfC, order=dfC['LotArea'].value_counts().head(20).index, palette='magma')
plt.xlabel('Count', fontsize=12)
plt.ylabel('Lot Area', fontsize=12)
plt.title('Top 20 Lot Areas', fontsize=16)
plt.show()

"""###c. Identify significant variables using p-values and Chi-Square values
If the p-value obtained is:
- 0.05 (the chosen α value), you reject the Null Hypothesis (H₀) and accept the Alternate Hypothesis (H₁). This indicates that the two categorical variables are dependent.
- 0.05, you accept the Null Hypothesis (H₀) and reject the Alternate Hypothesis (H₁). This suggests that the two categorical variables are independent.
"""

df.head()

!pip install researchpy

import researchpy as rp
import scipy.stats as stats

df.shape

#Get a count of the empty values of reach column
df.isna().sum()

#view statistic
df.describe()

rp.summary_cat(df[["LotArea", "SalePrice"]])

from scipy import stats

stats.chi2_contingency(pd.crosstab(df.LotArea,df.SalePrice))

"""
- Chi-square statistic: The calculated chi-square statistic is approximately 735,094.65. This statistic measures the discrepancy between the observed frequencies and the expected frequencies under the assumption of independence. A larger chi-square value indicates a stronger association between the variables.

- Degrees of freedom: The degrees of freedom is 709,664. It is calculated as the product of (number of rows - 1) and (number of columns - 1). It represents the number of independent observations used in the calculation of the chi-square statistic.

- p-value: The p-value is approximately 4.26e-99, which is extremely small. The p-value represents the probability of observing the given chi-square statistic (or more extreme values) under the null hypothesis of independence. In this case, the small p-value suggests strong evidence against the null hypothesis and indicates that there is a significant relationship between 'LotArea' and 'SalePrice'.

- Expected frequencies: The expected frequencies are provided as an array. These are the frequencies that would be expected if the variables were independent. They are based on the observed marginal totals and the assumption of independence.
"""

# Null Hypothesis: MoSold and SaleProice are independetn of each other
crosstab = pd.crosstab(df["LotArea"], df["SalePrice"])
crosstab

stats.chi2_contingency(crosstab)

pd.crosstab(df.SalePrice, df.LotArea, margins=True)

# Create the contingency table
contingency_table = pd.crosstab(df.SalePrice, df.LotArea, margins=True)

# Remove the "All" label from the row/column margins
contingency_table = contingency_table.drop("All", axis=0)
contingency_table = contingency_table.drop("All", axis=1)

# Select the top 20 rows and columns
top_20 = contingency_table.nlargest(20, columns=contingency_table.columns[-1])

# Plot the heatmap for the top 20 values
plt.figure(figsize=(12, 8))
sns.heatmap(top_20, cmap='YlGnBu', annot=True, fmt='d', cbar=True)
plt.xlabel('Lot Area', fontsize=12)
plt.ylabel('Sale Price', fontsize=12)
plt.title('Top 20 Contingency Table: Sale Price vs Lot Area', fontsize=16)
plt.show()

"""# 5. Combine all the significant categorical and numerical variables
jointplot which gives a plethora of information in a single plot.
- Scatter Plot
- Regression line fit to the data.
- Histogram and KDE of individual variables.
- Pearson correlation and p value.
"""

p1=df[['Neighborhood','SalePrice']]

p2=df[['LotArea','MoSold']]

p1.head(1)

p2.head(1)

pd.concat([p1,p2], axis=1)

# Concatenate p1 and p2 along axis 1
con_df = pd.concat([p1, p2], axis=1)

# Plot the concatenated DataFrame
con_df.plot(figsize=(10, 6))
plt.xlabel('X-axis', fontsize=12)
plt.ylabel('Y-axis', fontsize=12)
plt.title('Concatenated Data', fontsize=16)
plt.show()

# Jointplot which gives a plethora of information in a single plot.

sns.jointplot(x='LotArea', y='SalePrice', data=df, kind='reg', color='Purple')

"""#6. Plot box plot for the new dataset to find the variables with outliers"""

df.select_dtypes(['int', 'float'])

n=len(df.columns)

n

n//2+1

plt.figure(figsize=(14,6))
sns.boxplot(data=df,x='SalePrice')
plt.show()

plt.figure(figsize=(14,6))
sns.boxplot(data=df,x='OverallQual',y='SalePrice')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 8))
plt.xticks(rotation=45)
sns.boxplot(x='Neighborhood', y='SalePrice', data=df)

plt.xlabel('Neighborhood', fontsize=12)
plt.ylabel('Sale Price', fontsize=12)
plt.title('Boxplot of Sale Price by Neighborhood', fontsize=16)

plt.show()